# -*- coding: utf-8 -*-
"""Chennai_House_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPvGDHQpvrOoIMZXJDJgA69zYUo_wPmh

**Import the packages**
"""

import pandas as pd
import matplotlib.pyplot as plt
import time
import seaborn as sns
import numpy as np

"""**Read the Dataset**"""

data=pd.read_csv("train-chennai-sale.csv")

data.head()

"""**Column "PRT_ID" is not required to build the model. So, we will drop that column.**"""

data.drop(columns=['PRT_ID'],inplace=True)

"""# **DATA Cleaning**

**Analyse the Data Column by Column**
"""

data.columns

"""**AREA**"""

data.AREA.value_counts()

"""**In the Above we see that the Same Area name has been repeated, in Different Case,typo errors. We need Clean those records first.**"""

#Chrompt,Chrmpet,Chormpet These Values Needs to be Changed to "Chrompet"
data.AREA[data['AREA'].isin(["Chrompt","Chrmpet","Chormpet"])]="Chrompet"
#Karapakam This Values Needs to be Changed to "Karapakkam"
data.AREA[data['AREA']=="Karapakam"]="Karapakkam"
#KKNagar This Values Needs to be Changed to "KK Nagar"
data.AREA[data['AREA']=="KKNagar"]="KK Nagar"
#Velchery This Values Needs to be Changed to "Velachery"
data.AREA[data['AREA']=="Velchery"]="Velachery"
#Ana Nagar,Ann Nagar These Values Needs to be Changed to "Anna Nagar"
data.AREA[data['AREA'].isin(["Ana Nagar","Ann Nagar"])]="Anna Nagar"
#TNagar This Values Needs to be Changed to "T Nagar"
data.AREA[data['AREA']=="TNagar"]="T Nagar"
#Adyr This Values Needs to be Changed to "Adyar"
data.AREA[data['AREA']=="Adyr"]="Adyar"

data.AREA.value_counts()

"""**INT_SQFT**"""

data.INT_SQFT.value_counts()
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**DATE_SALE**"""

#DATE_SALE needs to conveted as Datatime type
data['DATE_SALE']=pd.to_datetime(data['DATE_SALE'])

"""**DIST_MAINROAD**"""

data.DIST_MAINROAD.value_counts()
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**N_BEDROOM**"""

data[data['N_BEDROOM'].isnull()]
# One record having the NaN value for N_BEDROOM and needs to be filled with "Median"value
data.N_BEDROOM[data['N_BEDROOM'].isnull()]=data.N_BEDROOM.median()

"""**N_BATHROOM**"""

data[data['N_BATHROOM'].isnull()]
# Five records having the NaN value for N_BEDROOM and needs to be filled with "Median"value
data.N_BATHROOM[data['N_BATHROOM'].isnull()]=data.N_BATHROOM.median()

"""**N_ROOM**"""

data[data['N_ROOM'].isna()]
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**SALE_COND**"""

data.SALE_COND.value_counts()

"""**In the Above we see that the Same Sale Condition name has been repeated, in Different Case,typo errors. We need Clean those records first.**"""

#AdjLand This Values Needs to be Changed to "Adj Land"
data.SALE_COND[data['SALE_COND']=="AdjLand"]="Adj Land"
#Partiall,PartiaLl These Values Needs to be Changed to "Partial"
data.SALE_COND[data['SALE_COND'].isin(["Partiall","PartiaLl"])]="Partial"
#Ab Normal This Values Needs to be Changed to "AbNormal"
data.SALE_COND[data['SALE_COND']=="Ab Normal"]="AbNormal"

data.SALE_COND.value_counts()

"""**PARK_FACIL**"""

data.PARK_FACIL.value_counts()
#"Noo" This Values Needs to be Changed to "No"
data.PARK_FACIL[data['PARK_FACIL']=="Noo"]="No"

data.PARK_FACIL.value_counts()

"""**DATE_BUILD**"""

#DATE_BUILD needs to conveted as Datatime type
data['DATE_BUILD']=pd.to_datetime(data['DATE_BUILD'])

"""**BUILDTYPE**"""

data.BUILDTYPE.value_counts()
#"Comercial" This Values Needs to be Changed to "Commercial"
data.BUILDTYPE[data['BUILDTYPE']=="Comercial"]="Commercial"
#"Other" This Values Needs to be Changed to "Others"
data.BUILDTYPE[data['BUILDTYPE']=="Other"]="Others"

data.BUILDTYPE.value_counts()

"""**UTILITY_AVAIL**"""

data.UTILITY_AVAIL.value_counts()
#"AllPub" This Values Needs to be Changed to "All Pub"
data.UTILITY_AVAIL[data['UTILITY_AVAIL']=="AllPub"]="All Pub"
#"NoSewr" This Values Needs to be Changed to "NoSeWa"
data.UTILITY_AVAIL[data['UTILITY_AVAIL']=="NoSeWr"]="NoSeWa"
data.UTILITY_AVAIL[data["UTILITY_AVAIL"]=="NoSewr "]="NoSeWa"

data.UTILITY_AVAIL.value_counts()

"""**STREET**"""

data.STREET.value_counts()
#"Pavd" This Values Needs to be Changed to "Paved"
data.STREET[data['STREET']=='Pavd']='Paved'
#"NoAccess" This Values Needs to be Changed to "No Access"
data.STREET[data['STREET']=='NoAccess']='No Access'

data.STREET.value_counts()

"""**MZZONE**"""

data.MZZONE.value_counts()
# Data Looks Fine No Cleaning Needed.

"""**QS_ROOMS**"""

data[data.QS_ROOMS.isna()]
# Data Looks Fine No Cleaning Needed.

"""**QS_BATHROOM**"""

data[data.QS_BATHROOM.isnull()]
# Data Looks Fine No Cleaning Needed.

"""**QS_BEDROOM**"""

data[data.QS_BEDROOM.isnull()]
# Data Looks Fine No Cleaning Needed.

"""**QS_OVERALL**"""

# Null records needs to be filled.
print("MEDIAN: " +str(data.QS_OVERALL.median()))
print("MEAN: "+str(round(data.QS_OVERALL.mean(),1)))
#Both Mean & MEDIAN values are same.So, we can use either Mean or Median.
data.QS_OVERALL[data.QS_OVERALL.isnull()]=round(data.QS_OVERALL.mean(),1)

"""**REG_FEE**"""

data[data.REG_FEE.isnull()]
# Data Looks fine, No Cleaning Required.

"""**COMMIS**"""

data[data.COMMIS.isnull()]
# Data Looks fine, No Cleaning Required.

"""**SALES_PRICE**"""

data[data.SALES_PRICE.isnull()]
# Data Looks fine, No Cleaning Required.

"""**Columns N_BEDROOM,N_BATHROOM,N_ROOM in type float but none of the data having decimal value, Hence will convert
columns to int type.**
"""

data.N_BEDROOM=data.N_BEDROOM.astype(int)
data.N_BATHROOM=data.N_BATHROOM.astype(int)
data.N_ROOM=data.N_ROOM.astype(int)

"""**DATA Cleaning Completed, Then we need to ENCODE the Data**"""

data.info()

"""# **ENCODING**

**AREA,SALE_COND,PARK_FACIL,BUILDTYPE,UTILITY_AVAIL,STREET,MZZONE seven columns contains catagorical data. These values needs to encoded.**

**AREA**
"""

# We plot AREA V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='AREA',y="SALES_PRICE",data=data,order=data.groupby('AREA')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['AREA'])

"""From the above plot we see that the AREA is having a linear relationship with the SALES_PRICE.Hence, we do the Label Encoding for this."""

data.AREA=data.AREA.map({"Karapakkam":1,"Adyar":2,"Chrompet":3,"Velachery":4,"KK Nagar":5,"Anna Nagar":6,"T Nagar":7})

data.AREA.value_counts()

"""**SALE_COND**"""

# We plot SALE_COND V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='SALE_COND',y="SALES_PRICE",data=data,order=data.groupby('SALE_COND')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['SALE_COND'])

"""From the above we see that the SALE_COND has no Relationship with the SALE_PRICE, Hence we can drop this feature. """

data.drop(columns=['SALE_COND'],inplace=True)

"""**PARK_FACIL**"""

# We plot PARK_FACIL V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='PARK_FACIL',y="SALES_PRICE",data=data,order=data.groupby('PARK_FACIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['PARK_FACIL'])

"""From the above we that PARK_FACIL has the Linear Relationship with SALES_PRICE, and We will do Label Encoding."""

data.PARK_FACIL=data.PARK_FACIL.map({"Yes":1,"No":0})

"""**BUILDTYPE**"""

# We plot BUILDTYPE V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='BUILDTYPE',y="SALES_PRICE",data=data,order=data.groupby('BUILDTYPE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['BUILDTYPE'])

"""From the above chart we see that, BUILDTYPE has the relationship with the SALES_PRICE but there is no linear relarionship, Hence we will One-Hot-Encoding for this."""

data=pd.concat([data,pd.get_dummies(data.BUILDTYPE)],axis=1)

data.drop(columns=['BUILDTYPE'],inplace=True)

"""**UTILITY_AVAIL**"""

# We plot UTILITY_AVAIL V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='UTILITY_AVAIL',y="SALES_PRICE",data=data,order=data.groupby('UTILITY_AVAIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['UTILITY_AVAIL'])

"""From the above chart we see that, UTILITY_AVAIL has the Linear-relationship with the SALES_PRICE, Hence we will do Label Encoding for this.


"""

data.UTILITY_AVAIL=data.UTILITY_AVAIL.map({'ELO':1,'NoSeWa':2,'All Pub':3})

data.UTILITY_AVAIL.value_counts()

"""**STREET**"""

# We plot STREET V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='STREET',y="SALES_PRICE",data=data,order=data.groupby('STREET')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['STREET'])

"""From the above chart we see that, STREET has the linear relationship with the SALES_PRICE, Hence we will One-Hot-Encoding for this."""

data.STREET=data.STREET.map({"No Access":1,"Paved":2,"Gravel":3})

"""**MZZONE**"""

# We plot MZZONE V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='MZZONE',y="SALES_PRICE",data=data,order=data.groupby('MZZONE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['MZZONE'])

"""From the above chart we see that, MZZONE has the Non-Linear-relationship with the SALES_PRICE, Hence we will do One-Hot Encoding for this."""

data=pd.concat([data,pd.get_dummies(data.MZZONE)],axis=1)

data.drop(columns=['MZZONE'],inplace=True)

"""**We Encode the DATE_BUILD & DATE_SALE with year alone.EX: 01-05-2017 to 2017.Because we are going to predict the Price range of the house, and price won't change drastically within a year.**"""

data['DATE_BUILD']=data['DATE_BUILD'].dt.year
data['DATE_SALE']=data['DATE_SALE'].dt.year

"""**We will Add two More columns, BUILDING_AGE (DATE_BUILD-DATE_SALE) & PRICE_PER_SQFT (SALE_PRICE/INT_SQFT)**"""

data['BUILDING_AGE']=data['DATE_SALE']-data['DATE_BUILD']
data['PRICE_PER_SQFT']=(data['SALES_PRICE']/data['INT_SQFT']).astype(int)

data.head()

"""**We added two columns and due to One-Hot Encoding few Columns got created,and our target column SALES_PRICE is in the middle of our table.To get the better look we will move the SALES_PRICE column to the last.**"""

sale=data.pop('SALES_PRICE')
data.head()

data.insert(28,"SALES_PRICE",sale)

data

"""# **Spliting the Training and Test data**"""

from sklearn.model_selection import train_test_split
features=data.columns[:28]
x=data[features]
y=data['SALES_PRICE']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
df=X_test

"""**Scaling the Data**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train) 
X_test = sc.transform(X_test)

"""**Function for DecisionTree Regression**

# **Model Building**
"""

from sklearn.tree import DecisionTreeRegressor

def decision_tree_reg(X_train,y_train,X_test,y_test):
  try:
    previous=-5
    model = DecisionTreeRegressor(max_depth = 1) 
    for i in range(1,51):
      model = DecisionTreeRegressor(max_depth = i) # initialise the model
      model.fit(X_train,y_train) #train the model
      # scoring the model - r2 squared
      test_score=model.score(X_test, y_test)
      print("Max Depth : ", i, " Train score : ", model.score(X_train,y_train)," Test Score: ",test_score)
      if test_score > model.score(X_train,y_train) or previous > test_score:
        model = DecisionTreeRegressor(max_depth = i-1)
        model.fit(X_train,y_train) 
        print("Max Depth : ", (i-1), " Train score : ", model.score(X_train,y_train)," Test Score: ",model.score(X_test, y_test))
        break
      previous=test_score
    return model
  except Exception as e:
    print(str(e))

"""**Function for RandomForest Regression**"""

from sklearn.ensemble import RandomForestRegressor
import math
def random_forest_reg(X_train,y_train):
  try:
    num_of_features=len(X_train[0])
    depth=round(math.log(num_of_features,2)) # Depth Calulated Based on log(No.of.Features)/log(2)
    random_forest_model = RandomForestRegressor(n_estimators = 100,max_depth=depth,max_features="sqrt")  
    random_forest_model.fit(X_train, y_train)
    return random_forest_model
  except Exception as e:
   print(str(e))

"""**Function for XGBoost Regression**"""

import xgboost as xgb
from sklearn.model_selection import cross_val_score
previous=0
previous_cross_val=-5
def xgb_reg(X_train,y_train):
  try:
    model = xgb.XGBRegressor(learning_rate = 0.01, n_estimators=100,verbosity = 0) # initialise the model
    for lr in [0.01,0.02,0.03,0.04,0.05,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:
      model = xgb.XGBRegressor(learning_rate = lr, n_estimators=100,verbosity = 0) # initialise the model
      model.fit(X_train,y_train) #train the model
      cross_val=np.mean(cross_val_score(model, X_train, y_train, cv=10))
      train_score=model.score(X_train,y_train)
      print("Learning rate : ", lr, " Train score : ", train_score, " Cross-Val score : ",cross_val )
      if (cross_val>0 and train_score>0 and previous_cross_val>0) and(cross_val > train_score or previous_cross_val>cross_val):
        model = xgb.XGBRegressor(learning_rate = previous, n_estimators=100,verbosity = 0)
        model.fit(X_train,y_train)
        print("Learning rate : ", previous, " Train score : ", model.score(X_train,y_train), " Cross-Val score : ",np.mean(cross_val_score(model, X_train, y_train, cv=10)) )
        break     
      previous=lr
      previous_cross_val=cross_val
    return model
  except Exception as e:
   print(str(e))

"""**Function to Evaluate the Models and Get the Best Model Based on R2 Score**"""

from sklearn.metrics import r2_score

def get_best_model_basedOn_r2(model_list,X_test,y_test):
  try:
    r2_score_list={}
    #Compute R2 for the all the models
    print("Compute R2 for the all the models")
    for i in model_list:
      y_pred = model_list[i].predict(X_test)
      r2_score_list[r2_score(y_test,y_pred)]=i
      print("Compare which model has best R2")
  #Compare which model has best R2:
    best_r2=-1
    first_model=0
    for i in r2_score_list:
        if first_model==0:
          best_r2=i
          first_model=1
        else:
          if i>best_r2:
            best_r2=i
    # return the Model which having the best R2
    print("\n\n******************************************************")
    print("Best Model based on R2_score is : ",r2_score_list[best_r2], " and its R2_Score is ",str(best_r2))
    print("\n******************************************************")

    return model_list[r2_score_list[best_r2]]
  except Exception as e:
    print(str(e))

model_list={}
print("Building DecisionTree Model")
model=decision_tree_reg(X_train,y_train,X_test,y_test)
model_list["DecisionTreeRegressor"]=model
print("Building Random Forest Model")
model=random_forest_reg(X_train,y_train)
model_list["RandomForestRegressor"]=model
print("Building XGBoost Model")
model=xgb_reg(X_train,y_train)
model_list["XGBRegressor"]=model

Best_Model=get_best_model_basedOn_r2(model_list,X_test,y_test)

"""Above Built the Multiple Models and Found the Best out of it based on R2_score.

**Lets Predict the Data with the Best Model identified**
"""

y_predict=Best_Model.predict(X_test)

df.columns

"""**Add the Act_Sales_Price,Predicted_Sales_Price & Difference_Per (to calulate the actual diffrence between actual and predicted price) to the Test DataFrame for Comparision.**"""

df["Act_Sales_Price"]=y_test
df["Predicted_Sales_Price"]=y_predict

"""# **Final Result**

**As per our Requirement lets Find the Price Range for the House.**
"""

Absolute_Difference=abs(df["Act_Sales_Price"]-df["Predicted_Sales_Price"])   
price_from=(df["Predicted_Sales_Price"]-Absolute_Difference).astype(int).astype(str)
price_to=(df["Predicted_Sales_Price"]+Absolute_Difference).astype(int).astype(str)
df["Difference_PER"]=round((abs(df["Act_Sales_Price"]-df["Predicted_Sales_Price"])/df["Act_Sales_Price"])*100,2)
df["Predicted_Price_Range"]=price_from+" to "+price_to

"""Above we find the Absoute diffrence between the Actual Sales price and Predicted Sales price. Then find the lower price(Predicted price - absoulte difference amount) and the upper price(Predicted price + absoulte difference amount) """

df.AREA=df.AREA.map({1:"Karapakkam",2:"Adyar",3:"Chrompet",4:"Velachery",5:"KK Nagar",6:"Anna Nagar",7:"T Nagar"})

df[['AREA','INT_SQFT','N_BEDROOM','DATE_BUILD','PARK_FACIL','DATE_SALE','Predicted_Price_Range','Difference_PER']]

Len=len(df)
print("Total.No.Of.Records: "+str(Len))
print("No.Of.Records (Diff % > 3): "+str(len(df[df.Difference_PER>=3])))
print("No.Of.Records (Diff % < 3): "+str(len(df[df.Difference_PER<3])))