# -*- coding: utf-8 -*-
"""Chennai_House_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPvGDHQpvrOoIMZXJDJgA69zYUo_wPmh

**Import the packages**
"""

import pandas as pd
import matplotlib.pyplot as plt
import time
import seaborn as sns
import numpy as np

"""**Read the Dataset**"""

data=pd.read_csv("train-chennai-sale.csv")

data.head()

"""**Column "PRT_ID" is not required to build the model. So, we will drop that column.**"""

data.drop(columns=['PRT_ID'],inplace=True)

"""# **DATA Cleaning**

**Analyse the Data Column by Column**
"""

data.columns

"""**AREA**"""

data.AREA.value_counts()

"""**In the Above we see that the Same Area name has been repeated, in Different Case,typo errors. We need Clean those records first.**"""

#Chrompt,Chrmpet,Chormpet These Values Needs to be Changed to "Chrompet"
data.AREA[data['AREA'].isin(["Chrompt","Chrmpet","Chormpet"])]="Chrompet"
#Karapakam This Values Needs to be Changed to "Karapakkam"
data.AREA[data['AREA']=="Karapakam"]="Karapakkam"
#KKNagar This Values Needs to be Changed to "KK Nagar"
data.AREA[data['AREA']=="KKNagar"]="KK Nagar"
#Velchery This Values Needs to be Changed to "Velachery"
data.AREA[data['AREA']=="Velchery"]="Velachery"
#Ana Nagar,Ann Nagar These Values Needs to be Changed to "Anna Nagar"
data.AREA[data['AREA'].isin(["Ana Nagar","Ann Nagar"])]="Anna Nagar"
#TNagar This Values Needs to be Changed to "T Nagar"
data.AREA[data['AREA']=="TNagar"]="T Nagar"
#Adyr This Values Needs to be Changed to "Adyar"
data.AREA[data['AREA']=="Adyr"]="Adyar"

data.AREA.value_counts()

"""**INT_SQFT**"""

data.INT_SQFT.value_counts()
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**DATE_SALE**"""

#DATE_SALE needs to conveted as Datatime type
data['DATE_SALE']=pd.to_datetime(data['DATE_SALE'])

"""**DIST_MAINROAD**"""

data.DIST_MAINROAD.value_counts()
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**N_BEDROOM**"""

data[data['N_BEDROOM'].isnull()]
# One record having the NaN value for N_BEDROOM and needs to be filled with "Median"value
data.N_BEDROOM[data['N_BEDROOM'].isnull()]=data.N_BEDROOM.median()

"""**N_BATHROOM**"""

data[data['N_BATHROOM'].isnull()]
# Five records having the NaN value for N_BEDROOM and needs to be filled with "Median"value
data.N_BATHROOM[data['N_BATHROOM'].isnull()]=data.N_BATHROOM.median()

"""**N_ROOM**"""

data[data['N_ROOM'].isna()]
# Data Looks Fine, No Cleaning/Imputation required for this Column

"""**SALE_COND**"""

data.SALE_COND.value_counts()

"""**In the Above we see that the Same Sale Condition name has been repeated, in Different Case,typo errors. We need Clean those records first.**"""

#AdjLand This Values Needs to be Changed to "Adj Land"
data.SALE_COND[data['SALE_COND']=="AdjLand"]="Adj Land"
#Partiall,PartiaLl These Values Needs to be Changed to "Partial"
data.SALE_COND[data['SALE_COND'].isin(["Partiall","PartiaLl"])]="Partial"
#Ab Normal This Values Needs to be Changed to "AbNormal"
data.SALE_COND[data['SALE_COND']=="Ab Normal"]="AbNormal"

data.SALE_COND.value_counts()

"""**PARK_FACIL**"""

data.PARK_FACIL.value_counts()
#"Noo" This Values Needs to be Changed to "No"
data.PARK_FACIL[data['PARK_FACIL']=="Noo"]="No"

data.PARK_FACIL.value_counts()

"""**DATE_BUILD**"""

#DATE_BUILD needs to conveted as Datatime type
data['DATE_BUILD']=pd.to_datetime(data['DATE_BUILD'])

"""**BUILDTYPE**"""

data.BUILDTYPE.value_counts()
#"Comercial" This Values Needs to be Changed to "Commercial"
data.BUILDTYPE[data['BUILDTYPE']=="Comercial"]="Commercial"
#"Other" This Values Needs to be Changed to "Others"
data.BUILDTYPE[data['BUILDTYPE']=="Other"]="Others"

data.BUILDTYPE.value_counts()

"""**UTILITY_AVAIL**"""

data.UTILITY_AVAIL.value_counts()
#"AllPub" This Values Needs to be Changed to "All Pub"
data.UTILITY_AVAIL[data['UTILITY_AVAIL']=="AllPub"]="All Pub"
#"NoSewr" This Values Needs to be Changed to "NoSeWa"
data.UTILITY_AVAIL[data['UTILITY_AVAIL']=="NoSeWr"]="NoSeWa"
data.UTILITY_AVAIL[data["UTILITY_AVAIL"]=="NoSewr "]="NoSeWa"

data.UTILITY_AVAIL.value_counts()

"""**STREET**"""

data.STREET.value_counts()
#"Pavd" This Values Needs to be Changed to "Paved"
data.STREET[data['STREET']=='Pavd']='Paved'
#"NoAccess" This Values Needs to be Changed to "No Access"
data.STREET[data['STREET']=='NoAccess']='No Access'

data.STREET.value_counts()

"""**MZZONE**"""

data.MZZONE.value_counts()
# Data Looks Fine No Cleaning Needed.

"""**QS_ROOMS**"""

data[data.QS_ROOMS.isna()]
# Data Looks Fine No Cleaning Needed.

"""**QS_BATHROOM**"""

data[data.QS_BATHROOM.isnull()]
# Data Looks Fine No Cleaning Needed.

"""**QS_BEDROOM**"""

data[data.QS_BEDROOM.isnull()]
# Data Looks Fine No Cleaning Needed.

"""**QS_OVERALL**"""

# Null records needs to be filled.
print("MEDIAN: " +str(data.QS_OVERALL.median()))
print("MEAN: "+str(round(data.QS_OVERALL.mean(),1)))
#Both Mean & MEDIAN values are same.So, we can use either Mean or Median.
data.QS_OVERALL[data.QS_OVERALL.isnull()]=round(data.QS_OVERALL.mean(),1)

"""**REG_FEE**"""

data[data.REG_FEE.isnull()]
# Data Looks fine, No Cleaning Required.

"""**COMMIS**"""

data[data.COMMIS.isnull()]
# Data Looks fine, No Cleaning Required.

"""**SALES_PRICE**"""

data[data.SALES_PRICE.isnull()]
# Data Looks fine, No Cleaning Required.

"""**Columns N_BEDROOM,N_BATHROOM,N_ROOM in type float but none of the data having decimal value, Hence will convert
columns to int type.**
"""

data.N_BEDROOM=data.N_BEDROOM.astype(int)
data.N_BATHROOM=data.N_BATHROOM.astype(int)
data.N_ROOM=data.N_ROOM.astype(int)

"""**DATA Cleaning Completed, Then we need to ENCODE the Data**"""

data.info()

"""# **ENCODING**

**AREA,SALE_COND,PARK_FACIL,BUILDTYPE,UTILITY_AVAIL,STREET,MZZONE seven columns contains catagorical data. These values needs to encoded.**

**AREA**
"""

# We plot AREA V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='AREA',y="SALES_PRICE",data=data,order=data.groupby('AREA')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['AREA'])

"""From the above plot we see that the AREA is having a linear relationship with the SALES_PRICE.Hence, we do the Label Encoding for this."""

data.AREA=data.AREA.map({"Karapakkam":1,"Adyar":2,"Chrompet":3,"Velachery":4,"KK Nagar":5,"Anna Nagar":6,"T Nagar":7})

data.AREA.value_counts()

"""**SALE_COND**"""

# We plot SALE_COND V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='SALE_COND',y="SALES_PRICE",data=data,order=data.groupby('SALE_COND')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['SALE_COND'])

"""From the above we see that the SALE_COND has Linear Relationship with the SALE_PRICE, Hence we can do Label Encoding for this feature. """

data.SALE_COND=data.SALE_COND.map({"Partial":1,"Family":2,"AbNormal":3,"Normal Sale":4,"Adj Land":5})

data.SALE_COND.value_counts()

"""**PARK_FACIL**"""

# We plot PARK_FACIL V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='PARK_FACIL',y="SALES_PRICE",data=data,order=data.groupby('PARK_FACIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['PARK_FACIL'])

"""From the above we that PARK_FACIL has the Linear Relationship with SALES_PRICE, and We will do Label Encoding."""

data.PARK_FACIL=data.PARK_FACIL.map({"Yes":1,"No":0})

"""**BUILDTYPE**"""

# We plot BUILDTYPE V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='BUILDTYPE',y="SALES_PRICE",data=data,order=data.groupby('BUILDTYPE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['BUILDTYPE'])

"""From the above chart we see that, BUILDTYPE has the relationship with the SALES_PRICE but there is no linear relarionship, Hence we will One-Hot-Encoding for this."""

data=pd.concat([data,pd.get_dummies(data.BUILDTYPE)],axis=1)

data.drop(columns=['BUILDTYPE'],inplace=True)

"""**UTILITY_AVAIL**"""

# We plot UTILITY_AVAIL V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='UTILITY_AVAIL',y="SALES_PRICE",data=data,order=data.groupby('UTILITY_AVAIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['UTILITY_AVAIL'])

"""From the above chart we see that, UTILITY_AVAIL has the Linear-relationship with the SALES_PRICE, Hence we will do Label Encoding for this.


"""

data.UTILITY_AVAIL=data.UTILITY_AVAIL.map({'ELO':1,'NoSeWa':2,'All Pub':3})

data.UTILITY_AVAIL.value_counts()

"""**N_BEDROOM**"""

sns.barplot(x='N_BEDROOM',y='SALES_PRICE',data=data,order=data.groupby('N_BEDROOM')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['N_BEDROOM'])

"""From the above chart we see that, N_BEDROOM has the relationship with the SALES_PRICE but there is non linear relarionship.

**N_ROOM**
"""

sns.barplot(x='N_ROOM',y='SALES_PRICE',data=data,order=data.groupby('N_ROOM')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['N_ROOM'])

"""From the above chart we see that, N_ROOM has the relationship with the SALES_PRICE but there is non linear relarionship.

**STREET**
"""

# We plot STREET V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='STREET',y="SALES_PRICE",data=data,order=data.groupby('STREET')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['STREET'])

"""From the above chart we see that, STREET has the linear relationship with the SALES_PRICE, Hence we will One-Hot-Encoding for this."""

data.STREET=data.STREET.map({"No Access":1,"Paved":2,"Gravel":3})

"""**MZZONE**"""

# We plot MZZONE V/S SALES Price to See the Relationship of the Feature.
sns.barplot(x='MZZONE',y="SALES_PRICE",data=data,order=data.groupby('MZZONE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['MZZONE'])

"""From the above chart we see that, MZZONE has the Slight-Linear-relationship with the SALES_PRICE, Hence we will do Label Encoding for this."""

data.MZZONE=data.MZZONE.map({"A":1,"C":2,"I":3,"RH":4,"RL":5,"RM":6})

data.MZZONE.value_counts()

"""**We Encode the DATE_BUILD & DATE_SALE with year alone.EX: 01-05-2017 to 2017.Because we are going to predict the Price range of the house, and price won't change drastically within a year.**"""

data['DATE_BUILD']=data['DATE_BUILD'].dt.year
data['DATE_SALE']=data['DATE_SALE'].dt.year

"""**We will Add two More columns, BUILDING_AGE (DATE_BUILD-DATE_SALE) & PRICE_PER_SQFT (SALE_PRICE/INT_SQFT)**"""

data['BUILDING_AGE']=data['DATE_SALE']-data['DATE_BUILD']
data['PRICE_PER_SQFT']=(data['SALES_PRICE']/data['INT_SQFT']).astype(int)

"""Lets Compare BUILDING_AGE v/s PRICE_PER_SQFT"""

sns.lineplot(x='BUILDING_AGE',y='PRICE_PER_SQFT',data=data)

"""In the above chart we see that the PRICE_PER_SQFT is getting reduced by age of the building. Building age is much related to the SALES_PRICE.

We added a new column 'PRICE_PER_SQFT', and we can use this as a Target instead of 'SALES_PRICE'. We will get better idea when we know the Price per Square feet House. Hence Will drop the SALES_PRICE column.
"""

data.drop(columns=['SALES_PRICE'],inplace=True)

"""In Data we are having COMMIS (Commission) & REG_FEE (Registration Fee) columns and these values will be calculated based on the Sales price of the house. Hence these columns are not require to build the model, So we can renove these columns."""

data.drop(columns=['COMMIS'],inplace=True)
data.drop(columns=['REG_FEE'],inplace=True)

data.head()

data.head()

"""# **Spliting the Training and Test data**"""

features=data.columns[:21]
features

from sklearn.model_selection import train_test_split
x=data[features]
y=data['PRICE_PER_SQFT']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
df=X_test

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train) 
X_test = sc.transform(X_test)

"""# **Model Building**

**Function for DecisionTree Regression**
"""

from sklearn.tree import DecisionTreeRegressor
def dt_model():
  dt_best_depth=pd.DataFrame(columns=['Cross_val','Learning_rate'])
  for i in range(1,21):
    model = DecisionTreeRegressor(max_depth = i) # initialise the model
    model.fit(X_train,y_train) #train the model
    # scoring the model - r2 squared
    dt_best_depth.loc[model.score(X_test, y_test)]=[model.score(X_test, y_test),i]
    print("Max Depth : ", i, " Train score : ", model.score(X_train,y_train)," Test Score: ",model.score(X_test, y_test))
    dt_best_depth
  best_depth=dt_best_depth.loc[dt_best_depth.Cross_val.max()][1]
  print("best_depth:",best_depth)
  descision_tree_model=DecisionTreeRegressor(max_depth=int(best_depth))
  descision_tree_model.fit(X_train,y_train)
  descision_tree_model.score(X_test,y_test)
  return descision_tree_model

"""**Function for RandomForest Regression**"""

from sklearn.ensemble import RandomForestRegressor
def rf_model(feature_count):
  max_dpth=round(np.log2(feature_count))
  random_forest_model = RandomForestRegressor(n_estimators = 100,max_depth=max_dpth,max_features="sqrt")
  #log(No.of.Features)/log(2) => Max Depth.
  random_forest_model.fit(X_train, y_train)
  random_forest_model.score(X_test,y_test)
  return random_forest_model

"""**Function for XGBoost Regression**"""

import xgboost as xgb
from sklearn.model_selection import cross_val_score
def xb_model():
  xgb_best_lr=pd.DataFrame(columns=['Cross_val','Learning_rate'])
  for lr in [0.01,0.02,0.03,0.04,0.05,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:
    model = xgb.XGBRegressor(learning_rate = lr, n_estimators=100,verbosity = 0) # initialise the model
    model.fit(X_train,y_train) #train the model
    print("Learning rate : ", lr, " Train score : ", model.score(X_train,y_train), " Cross-Val score : ", np.mean(cross_val_score(model, X_train, y_train, cv=10)))
    cross_val=round(np.mean(cross_val_score(model, X_train, y_train, cv=10)),2)
    xgb_best_lr.loc[cross_val]=[cross_val,lr]
  best_learn_rate=xgb_best_lr.loc[xgb_best_lr.Cross_val.max()][1]
  print('Best_Learn_Rate: ',best_learn_rate) 
  xgb_model = xgb.XGBRegressor(learning_rate = best_learn_rate,n_estimators=100, verbosity = 0)
  xgb_model.fit(X_train,y_train) 
  xgb_model.score(X_test, y_test) 
  return xgb_model

"""**Function to Evaluate the Models and Get the Best Model Based on R2 Score**"""

from sklearn.metrics import r2_score

def get_models_r2(model_list,X_test,y_test):
  try:
    r2_score_list={}
    #Compute R2 for the all the models
    print("Compute R2 for the all the models")
    for i in model_list:
      y_pred = model_list[i].predict(X_test)
      r2_score_list[r2_score(y_test,y_pred)]=i
    return r2_score_list
  
  except Exception as e:
    print(str(e))

"""**Lets Build the Models and find which is best among themselves based on R2 Score.**"""

def build_all_models():
  try:
    model_list={}
    print("Building Decision Tree Model")
    model_list["DecisionTreeRegressor"]=dt_model()
    print("Building RandomForest Model")
    model_list["RandomForestRegressor"]=rf_model(len(features))
    print("Building XGBoost Model")
    model_list["XGBRegressor"]=xb_model()

    r2_score_list=get_models_r2(model_list,X_test,y_test)
    #Compare which model has best R2:
    best_r2=-1
    first_model=0
    print("Compare which model has best R2")
    for i in r2_score_list:
        if first_model==0:
          best_r2=i
          first_model=1
        else:
          if i>best_r2:
            best_r2=i
    # return the Model which having the best R2
    print("\n\n******************************************************")
    print("Best Model based on R2_score is : ",r2_score_list[best_r2], " and its R2_Score is ",str(best_r2))
    print("\n******************************************************")

    
    x=r2_score_list.values()
    y=r2_score_list.keys()
    
    plt.title("Models v/s R2 Score")
    plt.xlabel("Models")
    plt.ylabel("R2-Score")
    plt.xticks(rotation=90)
    plt.bar(x,y)

    return model_list[r2_score_list[best_r2]]
  except Exception as error:
    print(error)

"""**Lets Build the Models and find which is best Among ThemSelves.**"""

Best_Model=build_all_models() # Build All Models and get the Best model based on R2 Score.

"""**Lets Predict the Data with the Best Model identified**"""

y_predict=Best_Model.predict(X_test)

x=df['AREA'].map({1:"Karapakkam",2:"Adyar",3:"Chrompet",4:"Velachery",5:"KK Nagar",6:"Anna Nagar",7:"T Nagar"})
y=y_test
plt.title("Actual Price v/s Sales Price")
plt.xticks(rotation=90)
plt.bar(x,y,color='red',label='Actual')
plt.bar(x,y_predict,label='Predicted')
plt.legend(loc='best')

"""# **Final Result**

**As per our Requirement lets Find the Price Range for the House.**
"""

df["Predicted_Sales_Price"]=y_predict.astype(int)

df.AREA=df.AREA.map({1:"Karapakkam",2:"Adyar",3:"Chrompet",4:"Velachery",5:"KK Nagar",6:"Anna Nagar",7:"T Nagar"})

"""To Calculate the Price Range, Lets get the Standard Deviation of the Predicted_Sales_Price by AREA wise and then subract the SD value with Predicted_Sales_Price to find the Lower Price Range and use the Predicted_Sales_Price value as the Upper Price range."""

df['Predicted_Sales_Price_Range']=0

round(df[df['AREA']=='Karapakkam'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='Karapakkam'] - 2038).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='Karapakkam']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='Karapakkam'].astype(str)

round(df[df['AREA']=='Adyar'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='Adyar'] - 2557).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='Adyar']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='Adyar'].astype(str)

round(df[df['AREA']=='Chrompet'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='Chrompet'] - 2013).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='Chrompet']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='Chrompet'].astype(str)

round(df[df['AREA']=='Velachery'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='Velachery'] - 2013).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='Velachery']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='Velachery'].astype(str)

round(df[df['AREA']=='KK Nagar'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='KK Nagar'] - 1381).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='KK Nagar']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='KK Nagar'].astype(str)

round(df[df['AREA']=='Anna Nagar'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='Anna Nagar'] - 1925).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='Anna Nagar']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='Anna Nagar'].astype(str)

round(df[df['AREA']=='T Nagar'].Predicted_Sales_Price.std())

df_lower_range=(df.Predicted_Sales_Price[df['AREA']=='T Nagar'] - 2055).astype(int)
df.Predicted_Sales_Price_Range[df['AREA']=='T Nagar']= df_lower_range.astype(str) +"  -  "+df.Predicted_Sales_Price[df['AREA']=='T Nagar'].astype(str)

df.head()

"""**Important Features Based on the Best Model**"""

Feature_imp=pd.DataFrame({'Important_Feature':Best_Model.feature_importances_},index=features)
Feature_imp=Feature_imp.Important_Feature.sort_values(ascending=False)
x_feature_imp=Feature_imp.index
y_feature_imp=Feature_imp
plt.title("Important Features based on Best Model")
plt.xticks(rotation=90)
plt.bar(x_feature_imp,y_feature_imp)

"""Builders should consider the below points while building the houses, this will help them to sell the houses quickly.

*   Choose the AREA which is popular or near to popular area.
*   Houses with Commercial space will be added advantage.
*   Parking Facility is must.
*   The street should have gravel or paving stone.
*   Provide Maximum Quality Space.
*   Build atleast two rooms and with attached Bathroom(s).
*   Make sure the availability of Basic utilites like Sewage,,Power-Backup,Lift,Security...
"""